{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!conda install -y gdown\n\n# !gdown --id 1JxsxrJsXCjfLhaSP4nJd4VX0NL1-BgAq\n# !gdown --id 1zRn-IXvWuzCqb87F2lxSUoyGdscMUjUW\n!gdown --id 1MrCOyR5NZxgn5Dfdkl3CJEoau1xsA9ab        #sixM dataset\n!gdown --id 1Y3yzenxiOOdYesEm8D9NELfFCDIyulW8        #checkpoint","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-01T04:35:05.47724Z","iopub.execute_input":"2021-08-01T04:35:05.477604Z","iopub.status.idle":"2021-08-01T04:36:37.315474Z","shell.execute_reply.started":"2021-08-01T04:35:05.477519Z","shell.execute_reply":"2021-08-01T04:36:37.314486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport csv\nfrom tqdm.notebook import tqdm\nimport re\nimport time\nimport joblib\n\nimport torch\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:36:37.317039Z","iopub.execute_input":"2021-08-01T04:36:37.317332Z","iopub.status.idle":"2021-08-01T04:36:39.356278Z","shell.execute_reply.started":"2021-08-01T04:36:37.3173Z","shell.execute_reply":"2021-08-01T04:36:39.355003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### configurations\nMAX_LEN = 512\nTRAIN_BATCH_SIZE = 8\nVALIDATION_BATCH_SIZE = 4\nEPOCHS = 1\nACCUMULATION = 2\nBERT_PATH = \"../input/bert-base-uncased/\"\nMODEL_PATH = \"../input/bert-base-uncased/\"\nOUTPUT_PATH = \"checkpoint3.pt\"\nSAVED_MODEL_PATH = \"checkpoint1(1).pt\"\n# TRAINING_FILE = \"/content/dataset/train.csv\"\nTOKENIZER = transformers.BertTokenizer.from_pretrained(\n    BERT_PATH, \n    do_lower_case = True\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:36:53.800047Z","iopub.execute_input":"2021-08-01T04:36:53.800425Z","iopub.status.idle":"2021-08-01T04:36:54.021745Z","shell.execute_reply.started":"2021-08-01T04:36:53.800391Z","shell.execute_reply":"2021-08-01T04:36:54.02088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data_loader\n\n#https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\nclass BERTDataset:\n    def __init__(self , sentence , target):\n        \"\"\"\n            sentence : list of strings(sentences)\n            target : list of ints\n        \"\"\"\n        self.sentence = sentence\n        self.target = target\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n\n    #total len of dataset\n    def __len__(self):\n        return len(self.sentence)\n\n    def __getitem__(self , idx):\n        sentence = str(self.sentence[idx])   #just to make sure everything is string and not ints or UTF\n        sentence = \" \".join(sentence.split())\n\n        #tokeizing the sentences\n        inputs = self.tokenizer.encode_plus(\n            text = sentence,\n            add_special_tokens = True,\n            max_length = self.max_len,\n            padding='max_length',\n            truncation = True\n            # return_attention_mask = True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        # print(f\"inputs {len(ids) } , mask {len(mask)}  len {len(sentence.split())}  target {self.target[idx]}\")\n\n        return {\n            'ids' : torch.tensor(ids  , dtype = torch.long),\n            'mask' : torch.tensor(mask , dtype = torch.long),\n            'targets' : torch.tensor(self.target[idx] , dtype = torch.long)\n        }\n\n        \n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:36:55.57245Z","iopub.execute_input":"2021-08-01T04:36:55.572801Z","iopub.status.idle":"2021-08-01T04:36:55.581234Z","shell.execute_reply.started":"2021-08-01T04:36:55.572768Z","shell.execute_reply":"2021-08-01T04:36:55.580302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model\n# import transformers\nimport torch.nn as nn\n\nclass BERTBaseUncased(nn.Module):\n    def __init__(self, target_size):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.bert_drop = nn.Dropout(p = 0.3)\n        self.out = nn.Linear(768 , target_size)   #change 1 to number of intnents and also add actication functions\n        self.soft = nn.Softmax()\n\n    def forward(self, ids , mask):\n        #out1 = (batch_size, sequence_length, 786) – Sequence of hidden-states at the output of the last layer of the model.\n        #out2 = (batch_size, 786) – Last layer hidden-state of the first token of the sequence (classification token) (?? not sure what this is)\n        #                         – Gives a vector of size 768 for each sample in batch\n        #https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n        _ , out2 = self.bert(\n            input_ids = ids,\n            attention_mask = mask,\n            return_dict=False\n            # token_type_ids = token_type_ids     #not sure if it's necessary for this task\n        )\n\n        bert_output = self.bert_drop(out2)\n        output = self.out(bert_output)\n        # soft_out = self.soft(output)\n        return output ","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:36:56.372226Z","iopub.execute_input":"2021-08-01T04:36:56.372557Z","iopub.status.idle":"2021-08-01T04:36:56.379153Z","shell.execute_reply.started":"2021-08-01T04:36:56.372526Z","shell.execute_reply":"2021-08-01T04:36:56.378033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#engine\n# !pip install tqdm \nfrom tqdm.notebook import tqdm\n\ndef loss_fn(outputs , targets):\n    # print(\"outputs : \" , outputs)\n    # print(\"targets : \", targets)\n    return nn.CrossEntropyLoss()(outputs , targets)\n\ndef train_fn(data_loader , model , optimizer , device , accumulation_steps,schedular):\n    model.train()\n    final_loss = 0\n    #loop through each batch\n    for batch_index , data_batch in tqdm(enumerate(data_loader) , total = len(data_loader)):\n        ids = data_batch[\"ids\"]\n        mask =  data_batch[\"mask\"]\n        targets = data_batch[\"targets\"]\n\n        ids = ids.to(device, dtype = torch.long)\n        mask = mask.to(device, dtype = torch.long)\n        targets = targets.to(device, dtype = torch.long)\n\n        optimizer.zero_grad()\n        outputs = model(\n            ids = ids,\n            mask = mask\n        )\n\n        # print(f\"inputs {len(ids) } , mask {len(mask)}  target {targets.shape}\")\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n#         print(f\"loss = {loss.item()}\")\n        final_loss += loss.item()\n        # optimizer.step()\n        # schedular.step()     \n\n        if (batch_index + 1) % accumulation_steps == 0:\n            optimizer.step()\n            schedular.step()\n            \n    return final_loss / len(data_loader)\n\n\ndef eval_fn(data_loader , model, device):\n    model.eval()\n    final_targets = []\n    final_outputs = []\n    final_loss = 0\n    \n    #loop through each batch\n    with torch.no_grad():   #??\n        for batch_index , data_batch in tqdm(enumerate(data_loader) , total = len(data_loader)):\n            ids = data_batch['ids']\n            mask =  data_batch['mask']\n            targets = data_batch['targets']\n\n            ids = ids.to(device, dtype = torch.long)\n            mask = mask.to(device, dtype = torch.long)\n            targets = targets.to(device, dtype = torch.long)\n\n            outputs = model(\n                ids = ids,\n                mask = mask\n            )\n            \n            loss = loss_fn(outputs, targets)\n            final_loss += loss.item()\n            \n            # print(\"eval output\" , outputs)\n            final_targets.extend(targets.cpu().detach().numpy().tolist())\n            final_outputs.extend(outputs.cpu().detach().numpy().argmax(axis = 1).tolist())    #change this in case of multiple outputs\n\n    return final_outputs , final_targets, final_loss / len(data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:36:56.994558Z","iopub.execute_input":"2021-08-01T04:36:56.994953Z","iopub.status.idle":"2021-08-01T04:36:57.010302Z","shell.execute_reply.started":"2021-08-01T04:36:56.994918Z","shell.execute_reply":"2021-08-01T04:36:57.00904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train\nimport pandas as pd\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import get_linear_schedule_with_warmup\nimport csv\nimport datetime\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\ndef run(df):\n#     df = pd.read_csv(TRAINING_FILE ,escapechar = \"\\\\\" , quoting = csv.QUOTE_NONE).fillna(\"none\")\n#     df = df.sample(n = 20000)\n#     print(df.head())\n\n#     key = df['BROWSE_NODE_ID'].value_counts()\n#     for index, row in tqdm(df.iterrows(), total = df.shape[0]):\n#         if(key[row['BROWSE_NODE_ID']] == 1):\n#             df.drop(index, inplace = True)\n\n#     le = LabelEncoder()\n#     df['BROWSE_NODE_ID'] = le.fit_transform(df['BROWSE_NODE_ID'])\n#     le_name_mapping = dict(zip( le.transform(le.classes_) , le.classes_))\n\n    df_train, df_valid = model_selection.train_test_split(\n        df,\n        test_size = 0.1,\n        random_state = 2000,\n        stratify = df.BROWSE_NODE_ID.values\n    )\n\n    df_train = df_train.reset_index(drop = True)\n    df_valid = df_valid.reset_index(drop = True)\n\n    # df_train = df_train.sample(n = 10000)\n    # df_test = df_test.sample(n = 1000)\n\n    train_dataset = BERTDataset(\n        sentence = df_train.text.values , \n        target = df_train.BROWSE_NODE_ID.values\n    )\n\n    valid_dataset = BERTDataset(\n        sentence = df_valid.text.values , \n        target = df_valid.BROWSE_NODE_ID.values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset ,\n        batch_size = TRAIN_BATCH_SIZE, \n        num_workers = 1\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset ,\n        batch_size = VALIDATION_BATCH_SIZE, \n        num_workers = 1\n    )\n    \n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(\"There are %s GPU's.\" %torch.cuda.device_count())\n        print(\"GPU Name: \" , torch.cuda.get_device_name(0))\n\n    else:\n        print(\"No GPU's Available :(\")\n        decive = torch.device(\"cpu\")\n    \n    model = BERTBaseUncased(df['BROWSE_NODE_ID'].nunique())\n    model.to(device)\n#     params = (list(model.named_parameters()))\n#     for p in params:\n#         print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n \n    optimizer_parameters = [\n        {\n            \"params\": [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.001,\n        },\n        {\n            \"params\": [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]   \n\n    num_train_steps = len(df_train)/TRAIN_BATCH_SIZE * EPOCHS\n    optimizer = AdamW(\n        optimizer_parameters,\n        lr = 2e-5\n    )\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n    )\n    \n    checkpoint = torch.load(SAVED_MODEL_PATH)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#     epoch = checkpoint['epoch']\n#     loss = checkpoint['loss']\n    \n    training_stats = []\n    best_accuracy = 0\n    for epoch in range(EPOCHS):\n        print(\"\")\n        print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n        print('Training...')\n        t0 = time.time()\n        \n        avg_train_loss = train_fn(train_data_loader , model , optimizer , device , ACCUMULATION , scheduler)\n        \n        training_time = format_time(time.time() - t0)\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epcoh took: {:}\".format(training_time))\n    \n        print(\"\")\n        print(\"Running Validation...\")\n        t0 = time.time()\n        \n        \n        outputs , targets , avg_val_loss = eval_fn(valid_data_loader , model, device )\n        accuracy = metrics.accuracy_score(targets , outputs)\n        \n        validation_time = format_time(time.time() - t0)\n        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n        print(\"  Validation took: {:}\".format(validation_time))\n        print(\"  Validation accuracy: {:}\".format(accuracy))\n        \n        training_stats.append(\n            {\n                'epoch': epoch + 1,\n                'Training Loss': avg_train_loss,\n                'Valid. Loss': avg_val_loss,\n                'Valid. Accur.': accuracy,\n                'Training Time': training_time,\n                'Validation Time': validation_time\n            }\n        )\n        \n        torch.save({\n            'epoch': 4,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': avg_val_loss,\n            'acc' : accuracy\n            }, OUTPUT_PATH)\n        \n        if(accuracy > best_accuracy):\n#             torch.save(model.state_dict(), OUTPUT_PATH)\n            print(f\"Accuracy Score = {accuracy}\")\n            best_accuracy = accuracy\n            \n    return training_stats\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:36:57.541188Z","iopub.execute_input":"2021-08-01T04:36:57.541517Z","iopub.status.idle":"2021-08-01T04:37:02.56889Z","shell.execute_reply.started":"2021-08-01T04:36:57.541484Z","shell.execute_reply":"2021-08-01T04:37:02.568024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #load data\n# df = pd.read_pickle('final_df')\n\n# # df.drop(['TITLE', 'DESCRIPTION', 'BULLET_POINTS', 'BRAND'] ,axis = 1,  inplace = True)\n# df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:02.570457Z","iopub.execute_input":"2021-08-01T04:37:02.570819Z","iopub.status.idle":"2021-08-01T04:37:02.575286Z","shell.execute_reply.started":"2021-08-01T04:37:02.570781Z","shell.execute_reply":"2021-08-01T04:37:02.574411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# len(df)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:02.577273Z","iopub.execute_input":"2021-08-01T04:37:02.577622Z","iopub.status.idle":"2021-08-01T04:37:02.590583Z","shell.execute_reply.started":"2021-08-01T04:37:02.577585Z","shell.execute_reply":"2021-08-01T04:37:02.58936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # def drop_sparse_classes(df):\n# #     unique_labels, label_counts = np.unique(df.BROWSE_NODE_ID, return_counts=True)\n# #     drop_labels = unique_labels[label_counts < 10]\n# #     _df = df.apply(lambda x: x['BROWSE_NODE_ID'] in unique_labels[label_counts < 10], axis = 1)\n# #     df_drop = df[_df]\n# #     return df_drop\n\n# def preprocess(df):\n#     key = df['BROWSE_NODE_ID'].value_counts()\n#     print(\"No. of labels having only one sample : \", key.value_counts()[1])\n    \n#     #So, we will have to remove those samples\n#     for index, row in tqdm(df.iterrows(), total = df.shape[0]):\n#         if(key[row['BROWSE_NODE_ID']] == 1):\n#             df.drop(index, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:10.196337Z","iopub.execute_input":"2021-08-01T04:37:10.196778Z","iopub.status.idle":"2021-08-01T04:37:10.201525Z","shell.execute_reply.started":"2021-08-01T04:37:10.19673Z","shell.execute_reply":"2021-08-01T04:37:10.200291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess(df)\n# # df = drop_sparse_classes(df)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:10.60105Z","iopub.execute_input":"2021-08-01T04:37:10.601401Z","iopub.status.idle":"2021-08-01T04:37:10.604825Z","shell.execute_reply.started":"2021-08-01T04:37:10.601371Z","shell.execute_reply":"2021-08-01T04:37:10.603898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.drop(['TITLE', 'DESCRIPTION', 'BULLET_POINTS', 'BRAND'] ,axis = 1,  inplace = True)\n# print(len(df))\n# print(len(df['BROWSE_NODE_ID'].value_counts()))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:10.783865Z","iopub.execute_input":"2021-08-01T04:37:10.784202Z","iopub.status.idle":"2021-08-01T04:37:10.78786Z","shell.execute_reply.started":"2021-08-01T04:37:10.784174Z","shell.execute_reply":"2021-08-01T04:37:10.786693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# _ , df_sixM = model_selection.train_test_split(\n#         df,\n#         test_size = 0.175,\n#         random_state = 2000,\n#         stratify = df.BROWSE_NODE_ID.values\n#     )","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:10.967203Z","iopub.execute_input":"2021-08-01T04:37:10.967521Z","iopub.status.idle":"2021-08-01T04:37:10.972467Z","shell.execute_reply.started":"2021-08-01T04:37:10.967491Z","shell.execute_reply":"2021-08-01T04:37:10.971461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess(df_sixM)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:11.364253Z","iopub.execute_input":"2021-08-01T04:37:11.364586Z","iopub.status.idle":"2021-08-01T04:37:11.368442Z","shell.execute_reply.started":"2021-08-01T04:37:11.364557Z","shell.execute_reply":"2021-08-01T04:37:11.367419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(df_sixM))\n# print(len(df_sixM['BROWSE_NODE_ID'].value_counts()))\n# df_sixM.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:11.570206Z","iopub.execute_input":"2021-08-01T04:37:11.570526Z","iopub.status.idle":"2021-08-01T04:37:11.57439Z","shell.execute_reply.started":"2021-08-01T04:37:11.570498Z","shell.execute_reply":"2021-08-01T04:37:11.57343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # !pip install joblib\n# import joblib","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:12.019287Z","iopub.execute_input":"2021-08-01T04:37:12.019615Z","iopub.status.idle":"2021-08-01T04:37:12.023548Z","shell.execute_reply.started":"2021-08-01T04:37:12.019586Z","shell.execute_reply":"2021-08-01T04:37:12.022543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def encoding(df):\n#     le = LabelEncoder()\n#     df['BROWSE_NODE_ID'] = le.fit_transform(df['BROWSE_NODE_ID'])\n#     decoder = dict(zip( le.transform(le.classes_) , le.classes_))\n#     return decoder","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:12.226951Z","iopub.execute_input":"2021-08-01T04:37:12.227276Z","iopub.status.idle":"2021-08-01T04:37:12.230471Z","shell.execute_reply.started":"2021-08-01T04:37:12.227246Z","shell.execute_reply":"2021-08-01T04:37:12.229461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# decoder = encoding(df_sixM)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:12.594672Z","iopub.execute_input":"2021-08-01T04:37:12.595021Z","iopub.status.idle":"2021-08-01T04:37:12.599482Z","shell.execute_reply.started":"2021-08-01T04:37:12.59499Z","shell.execute_reply":"2021-08-01T04:37:12.598617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# joblib.dump(decoder , 'decoder.joblib')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:13.045975Z","iopub.execute_input":"2021-08-01T04:37:13.046299Z","iopub.status.idle":"2021-08-01T04:37:13.050291Z","shell.execute_reply.started":"2021-08-01T04:37:13.046268Z","shell.execute_reply":"2021-08-01T04:37:13.049266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_sixM.to_pickle('df_sixM')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:13.504095Z","iopub.execute_input":"2021-08-01T04:37:13.504426Z","iopub.status.idle":"2021-08-01T04:37:13.507953Z","shell.execute_reply.started":"2021-08-01T04:37:13.504395Z","shell.execute_reply":"2021-08-01T04:37:13.507034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sixM = pd.read_pickle('df_sixM')\ndf_sixM.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:15.30793Z","iopub.execute_input":"2021-08-01T04:37:15.308265Z","iopub.status.idle":"2021-08-01T04:37:16.008756Z","shell.execute_reply.started":"2021-08-01T04:37:15.308234Z","shell.execute_reply":"2021-08-01T04:37:16.007925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_stats = run(df_sixM)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T04:37:17.91029Z","iopub.execute_input":"2021-08-01T04:37:17.910623Z","iopub.status.idle":"2021-08-01T12:02:56.78592Z","shell.execute_reply.started":"2021-08-01T04:37:17.910593Z","shell.execute_reply":"2021-08-01T12:02:56.784946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_stats","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:02:56.791775Z","iopub.execute_input":"2021-08-01T12:02:56.792191Z","iopub.status.idle":"2021-08-01T12:02:56.824372Z","shell.execute_reply.started":"2021-08-01T12:02:56.792144Z","shell.execute_reply":"2021-08-01T12:02:56.823045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade IPython","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:37:27.316818Z","iopub.execute_input":"2021-08-01T03:37:27.317136Z","iopub.status.idle":"2021-08-01T03:37:36.299911Z","shell.execute_reply.started":"2021-08-01T03:37:27.317109Z","shell.execute_reply":"2021-08-01T03:37:36.298913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython import display","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:37:39.135686Z","iopub.execute_input":"2021-08-01T03:37:39.136049Z","iopub.status.idle":"2021-08-01T03:37:39.143185Z","shell.execute_reply.started":"2021-08-01T03:37:39.136015Z","shell.execute_reply":"2021-08-01T03:37:39.142331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display.FileLink(r'checkpoint1.pt')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:37:47.374137Z","iopub.execute_input":"2021-08-01T03:37:47.374489Z","iopub.status.idle":"2021-08-01T03:37:47.380321Z","shell.execute_reply.started":"2021-08-01T03:37:47.374439Z","shell.execute_reply":"2021-08-01T03:37:47.379509Z"},"trusted":true},"execution_count":null,"outputs":[]}]}